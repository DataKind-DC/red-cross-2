{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Census Tract Information\n",
    "This Jupyter Notebook explores how to extract Census information for latitude-longitude and state pairings from the Red Cross Disaster Cases file. This notebook translates the method used in get_geocodes.R to Python (3.6).\n",
    "\n",
    "This script reverse geocodes lat-long coordinates to find census tracts.\n",
    "The overall approach finds the tract polygons for each lat-long coordinate and extracts census tract and block information from state tract shapefiles. Coordinates that were not localized to a tract in a state shapefile were reverse geocoded using the Phase 1 API technique.\n",
    "\n",
    "### Procedure:\n",
    "1. Download shapefiles from Census.gov, store in a folder\n",
    "2. Intersect lat-long coordinate with appropriate state shapefile to find Census polygon\n",
    "3. Extract Census information from Census tract\n",
    "\n",
    "### Inputs:\n",
    "- FTP TIGER/LINE shapefile, state census tract2010: ftp://ftp2.census.gov/geo/tiger/TIGER2010/TRACT/2010/ (Downloaded by the code below)\n",
    "- State FIPs code, state_FIPs_codes.txt: https://www.census.gov/geo/reference/ansi_statetables.html (under \"National FIPS and GNIS Codes File\" tab, Manually downloaded and saved as a TXT file)\n",
    "- 2009-2014_RedCross_DisasterCases.csv: Downloaded from Phase 1 data folder in the DKDC RC Google Drive\n",
    "\n",
    "### Outputs:\n",
    "- 2009_2014_RedCross_DisasterCases_with_census_data.csv: The original case data (2009-2014_RedCross_DisasterCases.csv) with census data as additional columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os\n",
    "import pandas as pd\n",
    "#import urllib\n",
    "#import urllib2 # download files\n",
    "\n",
    "# Set directories\n",
    "root_folder = os.path.abspath(\"../\")\n",
    "data_folder = os.path.abspath(\"../../\") + '/data'\n",
    "output_folder = root_folder + '/phase2/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherikasylvester/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (0,15,17,34,35,36,37,38,39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Load Red Cross data into dataframe\n",
    "redcross_disaster_cases = pd.read_csv(data_folder + '/2009-2014_RedCross_DisasterCases.csv',\n",
    "                                      encoding = \"ISO-8859-1\",\n",
    "                                      error_bad_lines = False)\n",
    "\n",
    "# Load State FIPs codes\n",
    "state_fips = pd.read_csv(data_folder + '/state_FIPs_codes.txt', sep=\"|\")\n",
    "\n",
    "# Make list of unique elements in 'esri_state' column\n",
    "dataset_state_list = list(redcross_disaster_cases['esri_state'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Shapefiles\n",
    "Download state shapefiles from Census.gov . First, URLs for each state is built using the state's FIPs number. Next, shapefiles from Census.gov are downloaded and stored in the folder 'shapefiles_tract'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build URLs to FTP shapefiles from Census.gov\n",
    "base_url = \"ftp://ftp2.census.gov/geo/tiger/TIGER2010/TRACT/2010/\"\n",
    "sf_names = []\n",
    "url = []\n",
    "counter = 1\n",
    "for abbrev in dataset_state_list:\n",
    "  if abbrev in state_fips['STUSAB'].tolist():\n",
    "    fips_temp = state_fips.loc[state_fips['STUSAB'] == abbrev,'STATE']\n",
    "    sf_names.append(\"tl_2010_\" + \"%02d\"%fips_temp + \"_tract10.zip\")\n",
    "    url.append(base_url + sf_names[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if shapefiles folder exists in the data folder, create if missing\n",
    "if not os.path.exists(data_folder + '/shapefiles_tract'):\n",
    "    os.makedirs(data_folder + '/shapefiles_tract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Shapefiles are stored in the RCP2 Google Drive folder. To download\n",
    "again from the original source see below:\n",
    "\n",
    "Download shapefiles for each state. There is some hang time/ timeouts\n",
    "occassionally. This occurs when loading in the browser also timeouts,\n",
    "so this is a FTP/ web hosting problem and not a code problem. Just\n",
    "restart loop every time it crashes...which happens often\n",
    "\n",
    "TO DO: find more stable links on Census.gov- these often time out\n",
    "\"\"\"\n",
    "\n",
    "# R code\n",
    "#for (i in 1:length(url)){\n",
    "#    if (sf_names[i] %!in% dir(paste(data_folder,'/shapefiles_tract',sep=''))){\n",
    "#      download.file(url[i],paste(data_folder,'/shapefiles_tract/',sf_names[i],sep = ''),mode = \"wb\")\n",
    "#      unzip(paste(data_folder,'/shapefiles_tract/',sf_names[i],sep=''),\n",
    "#            exdir = paste(data_folder,'/shapefiles_tract',sep=''))\n",
    "#    }\n",
    "#}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
